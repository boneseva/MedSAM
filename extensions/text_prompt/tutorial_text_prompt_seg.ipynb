{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical image segmentation with text-based prompts (Inference; Experimental Functionality)\n",
    "\n",
    "In this tutorial, we will show how to use text prompts for medical image segmentation. Algothgh SAM has not released the text-based segmentation, we can implement this function by ourselves since there are many pre-trained text encoders in the community. \n",
    "We used the [MICCAI FLARE 2022](https://zenodo.org/record/7860267) dataset as an example. This is an abdominal organ segmentation task and there are 13 different organs as shown in the following figure. \n",
    "\n",
    "![FLARE22](https://rumc-gcorg-p-public.s3.amazonaws.com/i/2022/03/29/20220309-FLARE22-Pictures-2.png)\n",
    "\n",
    "Assume that you have trained the model following the [training pipeline](https://github.com/bowang-lab/MedSAM/tree/main/extensions/text_prompt). If not, you can use this [checkpoint](https://drive.google.com/file/d/12YH-N6PAKayulhS99MBURVNpuQtVj98S/view?usp=sharing). \n",
    "\n",
    "Note: please install this package: `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "#!pip install git+https://github.com/bowang-lab/MedSAM.git\n",
    "#!pip install transformers"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "%matplotlib widget\n",
    "from segment_anything import sam_model_registry\n",
    "from ipywidgets import interact, widgets, FileUpload\n",
    "from segment_anything.modeling import PromptEncoder\n",
    "from transformers import CLIPTextModel\n",
    "from typing import Any, Optional, Tuple, Type\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import os\n",
    "join = os.path.join\n",
    "import cv2\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define text prompt encoder\n",
    "\n",
    "We used the pre-trained CLIP model as the text encoder. The prompt encoder in SAM maps all kinds of prompts to the same dimension (256) but the output dimension of CLIP model is 512. Thus, we added an additional projection layer `self.text_encoder_head = nn.Linear(512, embed_dim)` to the text encoder to align the dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "class TextPromptEncoder(PromptEncoder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        image_embedding_size: Tuple[int, int],\n",
    "        input_image_size: Tuple[int, int],\n",
    "        mask_in_chans: int = 1,\n",
    "        activation = nn.GELU,\n",
    "        ) -> None:\n",
    "        super().__init__(embed_dim, image_embedding_size, input_image_size, mask_in_chans, activation)\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "        text_encoder.requires_grad_(False)\n",
    "        self.text_encoder = text_encoder\n",
    "        self.text_encoder_head = nn.Linear(512, embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "        tokens: Optional[torch.Tensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Embeds different types of prompts, returning both sparse and dense\n",
    "        embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
    "            and labels to embed.\n",
    "          boxes (torch.Tensor or none): boxes to embed\n",
    "          masks (torch.Tensor or none): masks to embed\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
    "            BxNx(embed_dim), where N is determined by the number of input points\n",
    "            and boxes.\n",
    "          torch.Tensor: dense embeddings for the masks, in the shape\n",
    "            Bx(embed_dim)x(embed_H)x(embed_W)\n",
    "        \"\"\"\n",
    "        bs = self._get_batch_size(points, boxes, masks, tokens)\n",
    "        sparse_embeddings = torch.empty(\n",
    "            (bs, 0, self.embed_dim), device=self._get_device()\n",
    "        )\n",
    "        if points is not None:\n",
    "            coords, labels = points\n",
    "            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n",
    "        if boxes is not None:\n",
    "            box_embeddings = self._embed_boxes(boxes)\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
    "        if tokens is not None:\n",
    "            encoder_hidden_states = self.text_encoder(tokens)[0]\n",
    "            text_embeddings = self.text_encoder_head(encoder_hidden_states)\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, text_embeddings], dim=1)\n",
    "\n",
    "        if masks is not None:\n",
    "            dense_embeddings = self._embed_masks(masks)\n",
    "        else:\n",
    "            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n",
    "                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n",
    "            )\n",
    "\n",
    "        return sparse_embeddings, dense_embeddings\n",
    "    \n",
    "    def _get_batch_size(self, points, boxes, masks, tokens):\n",
    "        \"\"\"\n",
    "        Returns the batch size of the inputs.\n",
    "        \"\"\"\n",
    "        if points is not None:\n",
    "            return points[0].shape[0]\n",
    "        elif boxes is not None:\n",
    "            return boxes.shape[0]\n",
    "        elif masks is not None:\n",
    "            return masks.shape[0]\n",
    "        elif tokens is not None:\n",
    "            return tokens.shape[0]\n",
    "        else:\n",
    "            return 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the whole model\n",
    "\n",
    "The whole model architecture is the same as the bounding box-based version. The only difference is that the prompt encoder was changed to text encoder.\n",
    "Since MedSAM already provided an image encoder trained on a large scale medical image datasets, we can freeze it and only fine-tune the mask decoder during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "class MedSAMText(nn.Module):\n",
    "    def __init__(self, \n",
    "                image_encoder, \n",
    "                mask_decoder,\n",
    "                prompt_encoder,\n",
    "                device,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, image, tokens):\n",
    "        image_embedding = self.image_encoder(image) # (B, 256, 64, 64)\n",
    "        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=None,\n",
    "            masks=None,\n",
    "            tokens=tokens\n",
    "        )\n",
    "        low_res_logits, _ = self.mask_decoder(\n",
    "            image_embeddings=image_embedding, # (B, 256, 64, 64)\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "          ) # (B, 1, 256, 256)\n",
    "\n",
    "        return low_res_logits"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model\n",
    "\n",
    "Two models should be loaded:\n",
    "- the pre-trained MedSAM model\n",
    "- The text encoder and fine-tuned mask decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "medsam_ckpt_path = \"medsam_vit_b.pth\"\n",
    "device = \"cuda:0\"\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=medsam_ckpt_path)\n",
    "medsam_text_demo_checkpoint = \"medsam_text_prompt_flare22.pth\"\n",
    "text_prompt_encoder = TextPromptEncoder(\n",
    "    embed_dim = 256,\n",
    "    image_embedding_size = (64, 64),\n",
    "    input_image_size = (1024, 1024),\n",
    "    mask_in_chans = 1\n",
    ")\n",
    "medsam_text_demo = MedSAMText(\n",
    "    image_encoder=deepcopy(medsam_model.image_encoder),\n",
    "    mask_decoder=deepcopy(medsam_model.mask_decoder),\n",
    "    prompt_encoder=text_prompt_encoder,\n",
    "    device = device\n",
    ")\n",
    "medsam_text_demo_weights = torch.load(medsam_text_demo_checkpoint)\n",
    "for key in medsam_text_demo.state_dict().keys():\n",
    "    if not key.startswith('prompt_encoder.text_encoder.'):\n",
    "        medsam_text_demo.state_dict()[key].copy_(medsam_text_demo_weights[key])\n",
    "medsam_text_demo = medsam_text_demo.to(device)\n",
    "medsam_text_demo.eval()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a GUI for testing the text-based segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "class TextPromptDemo:\n",
    "    def __init__(self, model):\n",
    "        ## Requires transformers only if users would like to try text prompt\n",
    "        from transformers import CLIPTokenizer\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")   \n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.image = None\n",
    "        self.image_embeddings = None\n",
    "        self.img_size = None\n",
    "        self.img_name = None\n",
    "        self.gt = None\n",
    "        self.label_dict = {\n",
    "            1: [\"liver\"],\n",
    "            2: [\"right kidney\"],\n",
    "            3: [\"spleen\"],\n",
    "            4: [\"pancreas\"],\n",
    "            5: [\"aorta\"],\n",
    "            6: [\"inferior vena cava\", \"ivc\"],\n",
    "            7: [\"right adrenal gland\", \"rag\"],\n",
    "            8: [\"left adrenal gland\", \"lag\"],\n",
    "            9: [\"gallbladder\"],\n",
    "            10: [\"esophagus\"],\n",
    "            11: [\"stomach\"],\n",
    "            12: [\"duodenum\"],\n",
    "            13: [\"left kidney\"]\n",
    "        }\n",
    "        self.caption_label_dict = {}\n",
    "        for label_id, label_list in self.label_dict.items():\n",
    "            for label in label_list:\n",
    "                self.caption_label_dict[label] = label_id\n",
    "\n",
    "        avail_prompts = []\n",
    "        for v in self.label_dict.values():\n",
    "            avail_prompts += v\n",
    "        self.avail_prompts = \", \".join(avail_prompts)\n",
    "\n",
    "    def show_mask(self, mask, ax, random_color=False, alpha=0.95):\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([alpha])], axis=0)\n",
    "        else:\n",
    "            color = np.array([251/255, 252/255, 30/255, alpha])\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        ax.imshow(mask_image)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def infer(self, text):\n",
    "        tokens = self.tokenize_text(text).to(self.model.device)\n",
    "        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(\n",
    "            points = None,\n",
    "            boxes = None,\n",
    "            masks = None,\n",
    "            tokens = tokens\n",
    "        )\n",
    "        low_res_logits, _ = self.model.mask_decoder(\n",
    "            image_embeddings=self.image_embeddings, # (B, 256, 64, 64)\n",
    "            image_pe=self.model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "            )\n",
    "        low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "        low_res_pred = F.interpolate(\n",
    "            low_res_pred,\n",
    "            size = self.img_size,\n",
    "            mode = 'bilinear',\n",
    "            align_corners = False\n",
    "        )\n",
    "        low_res_pred = low_res_pred.detach().cpu().numpy().squeeze()\n",
    "        seg = np.uint8(low_res_pred > 0.5)\n",
    "\n",
    "        return seg\n",
    "\n",
    "    def show(self, fig_size=3, alpha=0.95):\n",
    "        assert self.image is not None, \"Please set image first.\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(fig_size, fig_size))\n",
    "        fig.canvas.header_visible = False\n",
    "        fig.canvas.footer_visible = False\n",
    "        fig.canvas.toolbar_visible = False\n",
    "        fig.canvas.resizable = False\n",
    "\n",
    "        #avil_ids = np.unique(self.gt)[1:]\n",
    "        print(\"Possible prompts:\", self.avail_prompts)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        ax.imshow(np.rot90(self.image, 2), cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "        text = widgets.Text(\n",
    "            value = '',\n",
    "            placeholder = 'Prompt',\n",
    "            description = 'Enter a prompt',\n",
    "            disabled = False,\n",
    "            style = {'description_width': 'initial'}\n",
    "        )\n",
    "        display(text)\n",
    "\n",
    "        def callback(wget):\n",
    "            caption = wget.value.lower().strip()\n",
    "            if len(fig.texts) > 0:\n",
    "                fig.texts[0].remove()\n",
    "            if caption not in self.avail_prompts:\n",
    "                fig.text(\n",
    "                    0.50,\n",
    "                    0.02,\n",
    "                    f\"Invalid prompt: {wget.value}\",\n",
    "                    horizontalalignment='center',\n",
    "                    wrap=True,\n",
    "                    color='r'\n",
    "                )\n",
    "            else:\n",
    "                ax.clear()\n",
    "                ax.imshow(np.rot90(self.image, 2), cmap='gray')\n",
    "                ax.axis('off')\n",
    "                seg = self.infer(caption)\n",
    "                self.show_mask(np.rot90(seg, 2), ax, random_color=False, alpha=alpha)\n",
    "\n",
    "        text.on_submit(callback)\n",
    "\n",
    "    def set_image(self, image):\n",
    "        self.image = image\n",
    "        self.img_size = image.shape[:2]\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.repeat(image[:,:,None], 3, -1)\n",
    "        image_preprocess = self.preprocess_image(image)\n",
    "        with torch.no_grad():\n",
    "            self.image_embeddings = self.model.image_encoder(image_preprocess)\n",
    "        \n",
    "    def preprocess_image(self, image):\n",
    "        img_resize = cv2.resize(\n",
    "            image,\n",
    "            (1024, 1024),\n",
    "            interpolation=cv2.INTER_CUBIC\n",
    "        )\n",
    "        # Resizing\n",
    "        img_resize = (img_resize - img_resize.min()) / np.clip(img_resize.max() - img_resize.min(), a_min=1e-8, a_max=None) # normalize to [0, 1], (H, W, 3\n",
    "        # convert the shape to (3, H, W)\n",
    "        assert np.max(img_resize)<=1.0 and np.min(img_resize)>=0.0, 'image should be normalized to [0, 1]'\n",
    "        img_tensor = torch.tensor(img_resize).float().permute(2, 0, 1).unsqueeze(0).to(self.model.device)\n",
    "\n",
    "        return img_tensor\n",
    "    \n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize text using CLIP tokenizer\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            text, max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\" \n",
    "        ).input_ids.squeeze(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# load demo nii data\n",
    "demo_file_nii = \"../../data/FLARE22Train/images/FLARE22_Tr_0046_0000.nii.gz\"\n",
    "file_sitk = sitk.ReadImage(demo_file_nii)\n",
    "image_data = sitk.GetArrayFromImage(file_sitk)\n",
    "# adjust window width and level\n",
    "lower_bound = -240.0\n",
    "upper_bound = 160.0\n",
    "image_data_pre = np.clip(image_data, lower_bound, upper_bound)\n",
    "# normalize to [0, 255]\n",
    "image_data_pre = (image_data_pre - np.min(image_data_pre))/(np.max(image_data_pre)-np.min(image_data_pre))*255.0\n",
    "image_data_pre = np.uint8(image_data_pre)\n",
    "# select middle slice; you can also manually select the slice that you want to test\n",
    "image_slice_id = int(image_data_pre.shape[0]/2)\n",
    "image_slice = image_data_pre[image_slice_id]\n",
    "# show the image to check whether it contains abdominal organs\n",
    "plt.imshow(np.rot90(image_slice, 2), cmap='gray')\n",
    "plt.show()\n",
    "plt.title('middle slice:'+str(image_slice_id))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input prompt in the following prompt box, e.g., liver and press `Enter`\n",
    "\n",
    "![text-seg](text_seg_demo.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "text_prompt_demo = TextPromptDemo(medsam_text_demo)\n",
    "text_prompt_demo.set_image(image_slice)\n",
    "text_prompt_demo.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing remarks\n",
    "\n",
    "This is still an experimental function at present. There is a large room for further improvements, such as open-vocabulary segmentation. Compared to other prompts, text-prompt provided a straightforward way to integrate partially labeled datasets since it eliminates the limation of fully labeled datasets in typical semantic segmentation tasks! We're excited about this research direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
